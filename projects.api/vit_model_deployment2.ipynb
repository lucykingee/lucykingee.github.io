{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMnrE6C1mJ1OzYbbG/Exn0v"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b4400bc6bcb048459e0d9f3d043fb652":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_20c42a12ad1345eb8ce6e13fe6728bed","IPY_MODEL_434d5a7dfb4340878028c9a0137afbcc","IPY_MODEL_3eea32d2c2084fc188c89d66b5476679"],"layout":"IPY_MODEL_a54f9676618342728fdfa9e22f437cc9"}},"20c42a12ad1345eb8ce6e13fe6728bed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aab141d3f16c4ceaa08449fbff226e37","placeholder":"‚Äã","style":"IPY_MODEL_efa548a40fd04a9080c6d5096af9dfd7","value":"100%"}},"434d5a7dfb4340878028c9a0137afbcc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d74afa87029d4a2bab2d96e4cf39f265","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_083577ce6d0949cda3dc87220f10785b","value":5}},"3eea32d2c2084fc188c89d66b5476679":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_744f9a4bfa2343b2bf8d06910904a2f0","placeholder":"‚Äã","style":"IPY_MODEL_a76e448b487e4bccac25944c22f7bf6d","value":"‚Äá5/5‚Äá[23:46&lt;00:00,‚Äá283.55s/it]"}},"a54f9676618342728fdfa9e22f437cc9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aab141d3f16c4ceaa08449fbff226e37":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efa548a40fd04a9080c6d5096af9dfd7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d74afa87029d4a2bab2d96e4cf39f265":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"083577ce6d0949cda3dc87220f10785b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"744f9a4bfa2343b2bf8d06910904a2f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a76e448b487e4bccac25944c22f7bf6d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["## Get various imports and helper functions\n","The code in the following cells prepares imports and data for the exercises below. They are taken from 09. PyTorch Model Deployment."],"metadata":{"id":"E1cB7lCAGogS"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qAm21nGPEFEP","executionInfo":{"status":"ok","timestamp":1721765975407,"user_tz":-60,"elapsed":61779,"user":{"displayName":"lucykingee ude","userId":"14499258343167677547"}},"outputId":"220dad1d-e6ba-4189-db66-a35c248ae4b6"},"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Couldn't find torchinfo... installing it.\n","[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\n","Cloning into 'pytorch-deep-learning'...\n","remote: Enumerating objects: 4056, done.\u001b[K\n","remote: Total 4056 (delta 0), reused 0 (delta 0), pack-reused 4056\u001b[K\n","Receiving objects: 100% (4056/4056), 646.90 MiB | 27.80 MiB/s, done.\n","Resolving deltas: 100% (2371/2371), done.\n","Updating files: 100% (248/248), done.\n"]}],"source":["# Continue with regular imports\n","import matplotlib.pyplot as plt\n","import torch\n","import torchvision\n","\n","from torch import nn\n","from torchvision import transforms\n","\n","# Try to get torchinfo, install it if it doesn't work\n","try:\n","    from torchinfo import summary\n","except:\n","    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n","    !pip install -q torchinfo\n","    from torchinfo import summary\n","\n","# Try to import the going_modular directory, download it from GitHub if it doesn't work\n","try:\n","    from going_modular.going_modular import data_setup, engine\n","    from helper_functions import download_data, set_seeds, plot_loss_curves\n","except:\n","    # Get the going_modular scripts\n","    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n","    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n","    !mv pytorch-deep-learning/going_modular .\n","    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n","    !rm -rf pytorch-deep-learning\n","    from going_modular.going_modular import data_setup, engine\n","    from helper_functions import download_data, set_seeds, plot_loss_curves"]},{"cell_type":"code","source":["\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"u1HZbOb5Gpbp","executionInfo":{"status":"ok","timestamp":1721765980427,"user_tz":-60,"elapsed":371,"user":{"displayName":"lucykingee ude","userId":"14499258343167677547"}},"outputId":"4230c9b4-ccca-4f4a-eeb7-38d1350e9671"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["### Preparing and training ViT feature extractor"],"metadata":{"id":"5700oDlLIAkZ"}},{"cell_type":"code","source":["def create_vit_model(num_classes:int=3,\n","                     seed:int=42):\n","    \"\"\"Creates a ViT-B/16 feature extractor model and transforms.\n","\n","    Args:\n","        num_classes (int, optional): number of target classes. Defaults to 3.\n","        seed (int, optional): random seed value for output layer. Defaults to 42.\n","\n","    Returns:\n","        model (torch.nn.Module): ViT-B/16 feature extractor model.\n","        transforms (torchvision.transforms): ViT-B/16 image transforms.\n","    \"\"\"\n","    # Create ViT_B_16 pretrained weights, transforms and model\n","    weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n","    transforms = weights.transforms()\n","    model = torchvision.models.vit_b_16(weights=weights)\n","\n","    # Freeze all layers in model\n","    for param in model.parameters():\n","        param.requires_grad = False\n","\n","    # Change classifier head to suit our needs (this will be trainable)\n","    torch.manual_seed(seed)\n","    model.heads = nn.Sequential(nn.Linear(in_features=768, # keep this the same as original model\n","                                          out_features=num_classes)) # update to reflect target number of classes\n","\n","    return model, transforms"],"metadata":{"id":"dFcjAqsCH_YS","executionInfo":{"status":"ok","timestamp":1721765986395,"user_tz":-60,"elapsed":371,"user":{"displayName":"lucykingee ude","userId":"14499258343167677547"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":[" ## Get function for making predictions and timing them"],"metadata":{"id":"YZlYxVEOHlpc"}},{"cell_type":"code","source":["import pathlib\n","import torch\n","\n","from PIL import Image\n","from timeit import default_timer as timer\n","from tqdm.auto import tqdm\n","from typing import List, Dict\n","\n","# 1. Create a function to return a list of dictionaries with sample, truth label, prediction, prediction probability and prediction time\n","def pred_and_store(paths: List[pathlib.Path],\n","                   model: torch.nn.Module,\n","                   transform: torchvision.transforms,\n","                   class_names: List[str],\n","                   device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\") -> List[Dict]:\n","\n","    # 2. Create an empty list to store prediction dictionaires\n","    pred_list = []\n","\n","    # 3. Loop through target paths\n","    for path in tqdm(paths):\n","\n","        # 4. Create empty dictionary to store prediction information for each sample\n","        pred_dict = {}\n","\n","        # 5. Get the sample path and ground truth class name\n","        pred_dict[\"image_path\"] = path\n","        class_name = path.parent.stem\n","        pred_dict[\"class_name\"] = class_name\n","\n","        # 6. Start the prediction timer\n","        start_time = timer()\n","\n","        # 7. Open image path\n","        img = Image.open(path)\n","\n","        # 8. Transform the image, add batch dimension and put image on target device\n","        transformed_image = transform(img).unsqueeze(0).to(device)\n","\n","        # 9. Prepare model for inference by sending it to target device and turning on eval() mode\n","        model = model.to(device)\n","        model.eval()\n","\n","        # 10. Get prediction probability, predicition label and prediction class\n","        with torch.inference_mode():\n","            pred_logit = model(transformed_image) # perform inference on target sample\n","            pred_prob = torch.softmax(pred_logit, dim=1) # turn logits into prediction probabilities\n","            pred_label = torch.argmax(pred_prob, dim=1) # turn prediction probabilities into prediction label\n","            pred_class = class_names[pred_label.cpu()] # hardcode prediction class to be on CPU\n","\n","            # 11. Make sure things in the dictionary are on CPU (required for inspecting predictions later on)\n","            pred_dict[\"pred_prob\"] = round(pred_prob.unsqueeze(0).max().cpu().item(), 4)\n","            pred_dict[\"pred_class\"] = pred_class\n","\n","            # 12. End the timer and calculate time per pred\n","            end_time = timer()\n","            pred_dict[\"time_for_pred\"] = round(end_time-start_time, 4)\n","\n","        # 13. Does the pred match the true label?\n","        pred_dict[\"correct\"] = class_name == pred_class\n","\n","        # 14. Add the dictionary to the list of preds\n","        pred_list.append(pred_dict)\n","\n","    # 15. Return list of prediction dictionaries\n","    return pred_list"],"metadata":{"id":"O0Q421gvHsUx","executionInfo":{"status":"ok","timestamp":1721765993006,"user_tz":-60,"elapsed":365,"user":{"displayName":"lucykingee ude","userId":"14499258343167677547"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Create another ViT feature model instance\n","vit_food101_20_percent, vit_transforms = create_vit_model(num_classes=101)\n","\n","# Print ViT model summary (uncomment for full output)\n","from torchinfo import summary\n","summary(vit_food101_20_percent,\n","        input_size=(1, 3, 224, 224),\n","        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","        col_width=20,\n","        row_settings=[\"var_names\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZgX8d2pvHeE5","executionInfo":{"status":"ok","timestamp":1721766005953,"user_tz":-60,"elapsed":6311,"user":{"displayName":"lucykingee ude","userId":"14499258343167677547"}},"outputId":"00193442-98d2-4739-e419-10bd021c8319"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 330M/330M [00:02<00:00, 159MB/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["============================================================================================================================================\n","Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n","============================================================================================================================================\n","VisionTransformer (VisionTransformer)                        [1, 3, 224, 224]     [1, 101]             768                  Partial\n","‚îú‚îÄConv2d (conv_proj)                                         [1, 3, 224, 224]     [1, 768, 14, 14]     (590,592)            False\n","‚îú‚îÄEncoder (encoder)                                          [1, 197, 768]        [1, 197, 768]        151,296              False\n","‚îÇ    ‚îî‚îÄDropout (dropout)                                     [1, 197, 768]        [1, 197, 768]        --                   --\n","‚îÇ    ‚îî‚îÄSequential (layers)                                   [1, 197, 768]        [1, 197, 768]        --                   False\n","‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_0)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n","‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_1)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n","‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_2)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n","‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_3)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n","‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_4)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n","‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_5)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n","‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_6)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n","‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_7)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n","‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_8)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n","‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_9)                   [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n","‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_10)                  [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n","‚îÇ    ‚îÇ    ‚îî‚îÄEncoderBlock (encoder_layer_11)                  [1, 197, 768]        [1, 197, 768]        (7,087,872)          False\n","‚îÇ    ‚îî‚îÄLayerNorm (ln)                                        [1, 197, 768]        [1, 197, 768]        (1,536)              False\n","‚îú‚îÄSequential (heads)                                         [1, 768]             [1, 101]             --                   True\n","‚îÇ    ‚îî‚îÄLinear (0)                                            [1, 768]             [1, 101]             77,669               True\n","============================================================================================================================================\n","Total params: 85,876,325\n","Trainable params: 77,669\n","Non-trainable params: 85,798,656\n","Total mult-adds (M): 172.54\n","============================================================================================================================================\n","Input size (MB): 0.60\n","Forward/backward pass size (MB): 104.09\n","Params size (MB): 229.50\n","Estimated Total Size (MB): 334.19\n","============================================================================================================================================"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# Create Food101 training data transforms (only perform data augmentation on the training images)\n","food101_train_transforms = torchvision.transforms.Compose([\n","    torchvision.transforms.TrivialAugmentWide(),\n","    vit_transforms,\n","])\n","\n","food101_train_transforms"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jM5E5u74HWM8","executionInfo":{"status":"ok","timestamp":1721766022602,"user_tz":-60,"elapsed":372,"user":{"displayName":"lucykingee ude","userId":"14499258343167677547"}},"outputId":"bcd93068-c8c4-4cbf-ce37-707e867ff94f"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Compose(\n","    TrivialAugmentWide(num_magnitude_bins=31, interpolation=InterpolationMode.NEAREST, fill=None)\n","    ImageClassification(\n","    crop_size=[224]\n","    resize_size=[256]\n","    mean=[0.485, 0.456, 0.406]\n","    std=[0.229, 0.224, 0.225]\n","    interpolation=InterpolationMode.BILINEAR\n",")\n",")"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["from torchvision import datasets\n","\n","# Setup data directory\n","from pathlib import Path\n","data_dir = Path(\"data\")\n","\n","# Get training data (~750 images x 101 food classes)\n","train_data = datasets.Food101(root=data_dir, # path to download data to\n","                              split=\"train\", # dataset split to get\n","                              transform=food101_train_transforms, # perform data augmentation on training data\n","                              download=True) # want to download?\n","\n","# Get testing data (~250 images x 101 food classes)\n","test_data = datasets.Food101(root=data_dir,\n","                             split=\"test\",\n","                             transform=vit_transforms, # perform normal ViT transforms on test data\n","                             download=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZkkaStWlIRKi","executionInfo":{"status":"ok","timestamp":1721766290885,"user_tz":-60,"elapsed":264645,"user":{"displayName":"lucykingee ude","userId":"14499258343167677547"}},"outputId":"32da9671-929c-46b4-cf2f-a9c1aa6cb3dd"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://data.vision.ee.ethz.ch/cvl/food-101.tar.gz to data/food-101.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4996278331/4996278331 [02:39<00:00, 31410231.88it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/food-101.tar.gz to data\n"]}]},{"cell_type":"code","source":["\n","# Get Food101 class names\n","food101_class_names = train_data.classes\n","\n","# View the first 10\n","food101_class_names[:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8boQPyz2IXZ5","executionInfo":{"status":"ok","timestamp":1721766290885,"user_tz":-60,"elapsed":6,"user":{"displayName":"lucykingee ude","userId":"14499258343167677547"}},"outputId":"d2689e1e-3139-4347-8bb5-b01b6681f926"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['apple_pie',\n"," 'baby_back_ribs',\n"," 'baklava',\n"," 'beef_carpaccio',\n"," 'beef_tartare',\n"," 'beet_salad',\n"," 'beignets',\n"," 'bibimbap',\n"," 'bread_pudding',\n"," 'breakfast_burrito']"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["### Create Food101 20% data splits\n","Want to split whole Food101 dataset into:\n","\n","* Train set: 20% of whole original Food101 train dataset\n","* Test set: 20% of whole original Food101 test dataset"],"metadata":{"id":"Qyo6YwHkFCgk"}},{"cell_type":"code","source":["def split_dataset(dataset:torchvision.datasets, split_size:float=0.2, seed:int=42):\n","    \"\"\"Randomly splits a given dataset into two proportions based on split_size and seed.\n","\n","    Args:\n","        dataset (torchvision.datasets): A PyTorch Dataset, typically one from torchvision.datasets.\n","        split_size (float, optional): How much of the dataset should be split?\n","            E.g. split_size=0.2 means there will be a 20% split and an 80% split. Defaults to 0.2.\n","        seed (int, optional): Seed for random generator. Defaults to 42.\n","\n","    Returns:\n","        tuple: (random_split_1, random_split_2) where random_split_1 is of size split_size*len(dataset) and\n","            random_split_2 is of size (1-split_size)*len(dataset).\n","    \"\"\"\n","    # Create split lengths based on original dataset length\n","    length_1 = int(len(dataset) * split_size) # desired length\n","    length_2 = len(dataset) - length_1 # remaining length\n","\n","    # Print out info\n","    print(f\"[INFO] Splitting dataset of length {len(dataset)} into splits of size: {length_1} ({int(split_size*100)}%), {length_2} ({int((1-split_size)*100)}%)\")\n","\n","    # Create splits with given random seed\n","    random_split_1, random_split_2 = torch.utils.data.random_split(dataset,\n","                                                                   lengths=[length_1, length_2],\n","                                                                   generator=torch.manual_seed(seed)) # set the random seed for reproducible splits\n","    return random_split_1, random_split_2\n",""],"metadata":{"id":"wW4N0er9IpwO","executionInfo":{"status":"ok","timestamp":1721766298488,"user_tz":-60,"elapsed":351,"user":{"displayName":"lucykingee ude","userId":"14499258343167677547"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Create training 20% split of Food101\n","train_data_food101_20_percent, _ = split_dataset(dataset=train_data,\n","                                                 split_size=0.2)\n","\n","# Create testing 20% split of Food101\n","test_data_food101_20_percent, _ = split_dataset(dataset=test_data,\n","                                                split_size=0.2)\n","\n","len(train_data_food101_20_percent), len(test_data_food101_20_percent)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0xXJvH4nIrBb","executionInfo":{"status":"ok","timestamp":1721766301501,"user_tz":-60,"elapsed":412,"user":{"displayName":"lucykingee ude","userId":"14499258343167677547"}},"outputId":"ff9f46ed-1332-4bbd-8192-f837ddda81bc"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Splitting dataset of length 75750 into splits of size: 15150 (20%), 60600 (80%)\n","[INFO] Splitting dataset of length 25250 into splits of size: 5050 (20%), 20200 (80%)\n"]},{"output_type":"execute_result","data":{"text/plain":["(15150, 5050)"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["## Create DataLoaders for Food101 20 percent data"],"metadata":{"id":"QE2bJgz-GnrI"}},{"cell_type":"code","source":["\n","# Turn torch Datasets into DataLoaders\n","from torch.utils.data import DataLoader\n","\n","BATCH_SIZE = 32\n","NUM_WORKERS = 2\n","train_dataloader_food101 = DataLoader(train_data_food101_20_percent,\n","                                      batch_size=BATCH_SIZE,\n","                                      shuffle=True,\n","                                      num_workers=NUM_WORKERS)\n","\n","test_dataloader_food101 = DataLoader(test_data_food101_20_percent,\n","                                     batch_size=BATCH_SIZE,\n","                                     shuffle=False,\n","                                     num_workers=NUM_WORKERS)\n","\n","len(train_dataloader_food101), len(test_dataloader_food101)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e2FgeOV1IyXh","executionInfo":{"status":"ok","timestamp":1721766307004,"user_tz":-60,"elapsed":375,"user":{"displayName":"lucykingee ude","userId":"14499258343167677547"}},"outputId":"21c95e88-df60-4fff-f635-789f804dd844"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(474, 158)"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["### Train ViT feature extractor on 20% of Food101 data\n","Note: The cell below may take 15 mins to run on Google Colab (due to ~15,000 training images and ~5000 testing images)."],"metadata":{"id":"sp7bOowtI-6y"}},{"cell_type":"code","source":["# Loss function\n","loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(params=vit_food101_20_percent.parameters(),\n","                             lr=1e-3)\n","\n","# Train the model\n","set_seeds()\n","vit_food101_20_percent_results = engine.train(model=vit_food101_20_percent,\n","                                              train_dataloader=train_dataloader_food101,\n","                                              test_dataloader=test_dataloader_food101,\n","                                              epochs=5,\n","                                              optimizer=optimizer,\n","                                              loss_fn=loss_fn,\n","                                              device=device)\n",""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":136,"referenced_widgets":["b4400bc6bcb048459e0d9f3d043fb652","20c42a12ad1345eb8ce6e13fe6728bed","434d5a7dfb4340878028c9a0137afbcc","3eea32d2c2084fc188c89d66b5476679","a54f9676618342728fdfa9e22f437cc9","aab141d3f16c4ceaa08449fbff226e37","efa548a40fd04a9080c6d5096af9dfd7","d74afa87029d4a2bab2d96e4cf39f265","083577ce6d0949cda3dc87220f10785b","744f9a4bfa2343b2bf8d06910904a2f0","a76e448b487e4bccac25944c22f7bf6d"]},"id":"f80dv_lTI6_5","executionInfo":{"status":"ok","timestamp":1721767737841,"user_tz":-60,"elapsed":1427373,"user":{"displayName":"lucykingee ude","userId":"14499258343167677547"}},"outputId":"495261c5-6e1b-4980-e0e6-643b3ca4983a"},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/5 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4400bc6bcb048459e0d9f3d043fb652"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 1 | train_loss: 2.8532 | train_acc: 0.4279 | test_loss: 2.0665 | test_acc: 0.6257\n","Epoch: 2 | train_loss: 2.2209 | train_acc: 0.5931 | test_loss: 1.9249 | test_acc: 0.6657\n","Epoch: 3 | train_loss: 2.0587 | train_acc: 0.6439 | test_loss: 1.8675 | test_acc: 0.6981\n","Epoch: 4 | train_loss: 1.9557 | train_acc: 0.6810 | test_loss: 1.8514 | test_acc: 0.6989\n","Epoch: 5 | train_loss: 1.8883 | train_acc: 0.7009 | test_loss: 1.8297 | test_acc: 0.7078\n"]}]},{"cell_type":"markdown","source":["## Saving and loading FoodVision Big"],"metadata":{"id":"JIvHjhT_KJ96"}},{"cell_type":"code","source":["from going_modular.going_modular import utils\n","\n","# Create a model path\n","vit_food101_model_path = \"pretrained_vit_feature_extractor_food101_20_percent.pth\"\n","\n","# Save FoodVision Big model\n","utils.save_model(model=vit_food101_20_percent,\n","                 target_dir=\"models\",\n","                 model_name=vit_food101_model_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lUlgiAD2KKv5","executionInfo":{"status":"ok","timestamp":1721767784778,"user_tz":-60,"elapsed":1351,"user":{"displayName":"lucykingee ude","userId":"14499258343167677547"}},"outputId":"974e5549-a5d5-4732-c64d-688ecaebc61c"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Saving model to: models/pretrained_vit_feature_extractor_food101_20_percent.pth\n"]}]},{"cell_type":"code","source":["# Create Food101 compatible EffNetB2 instance\n","loaded_vit_food101, vit_transforms = create_vit_model(num_classes=101)\n","\n","# Load the saved model's state_dict()\n","loaded_vit_food101.load_state_dict(torch.load(\"models/pretrained_vit_feature_extractor_food101_20_percent.pth\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FrXWY0vuKyiA","executionInfo":{"status":"ok","timestamp":1721767792758,"user_tz":-60,"elapsed":4345,"user":{"displayName":"lucykingee ude","userId":"14499258343167677547"}},"outputId":"d7e8dc00-8ff4-4052-c2f8-b7d1b4795622"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["### Checking FoodVision Big model size"],"metadata":{"id":"PvF5Jq3JLoyo"}},{"cell_type":"code","source":["from pathlib import Path\n","\n","# Get the model size in bytes then convert to megabytes\n","pretrained_vit_food101_model_size = Path(\"models\", vit_food101_model_path).stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly)\n","print(f\"Pretrained vit feature extractor Food101 model size: {pretrained_vit_food101_model_size} MB\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2sF3w3CqLq2h","executionInfo":{"status":"ok","timestamp":1721767797345,"user_tz":-60,"elapsed":369,"user":{"displayName":"lucykingee ude","userId":"14499258343167677547"}},"outputId":"ef5ca0ce-1b01-491b-bc2a-d1129149c428"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Pretrained vit feature extractor Food101 model size: 327 MB\n"]}]},{"cell_type":"markdown","source":["## Turning our FoodVision Big model into a deployable app"],"metadata":{"id":"RzRMcEUsMGGi"}},{"cell_type":"code","source":["from pathlib import Path\n","\n","# Create FoodVision Big demo path\n","foodvision_big_demo_path = Path(\"demos/foodvision_big2/\")\n","\n","# Make FoodVision Big demo directory\n","foodvision_big_demo_path.mkdir(parents=True, exist_ok=True)\n","\n","# Make FoodVision Big demo examples directory\n","(foodvision_big_demo_path / \"examples\").mkdir(parents=True, exist_ok=True)"],"metadata":{"id":"W13FDMS7MHir","executionInfo":{"status":"ok","timestamp":1721767800921,"user_tz":-60,"elapsed":382,"user":{"displayName":"lucykingee ude","userId":"14499258343167677547"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["###  Downloading an example image and moving it to the examples directory"],"metadata":{"id":"oD0GgupEMhAe"}},{"cell_type":"code","source":["# Create path to Food101 class names\n","foodvision_big_class_names_path = foodvision_big_demo_path / \"class_names.txt\"\n","\n","# Write Food101 class names list to file\n","with open(foodvision_big_class_names_path, \"w\") as f:\n","    print(f\"[INFO] Saving Food101 class names to {foodvision_big_class_names_path}\")\n","    f.write(\"\\n\".join(food101_class_names)) # leave a new line between each class"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-965gVAtMiVp","executionInfo":{"status":"ok","timestamp":1721767803627,"user_tz":-60,"elapsed":4,"user":{"displayName":"lucykingee ude","userId":"14499258343167677547"}},"outputId":"461d6952-42f5-489a-c931-feff43483c62"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Saving Food101 class names to demos/foodvision_big2/class_names.txt\n"]}]},{"cell_type":"code","source":["# Open Food101 class names file and read each line into a list\n","with open(foodvision_big_class_names_path, \"r\") as f:\n","    food101_class_names_loaded = [food.strip() for food in  f.readlines()]\n","\n","# View the first 5 class names loaded back in\n","food101_class_names_loaded[:5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4PMgIVSiM5yh","executionInfo":{"status":"ok","timestamp":1721767808319,"user_tz":-60,"elapsed":361,"user":{"displayName":"lucykingee ude","userId":"14499258343167677547"}},"outputId":"35def744-b6b5-488f-93e7-c720bc26a512"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare']"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["### Turning our FoodVision Big model into a Python script (model.py)"],"metadata":{"id":"DGwppYL2NPX3"}},{"cell_type":"code","source":["%%writefile demos/foodvision_big2/model.py\n","import torch\n","import torchvision\n","\n","from torch import nn\n","\n","\n","def create_vit_model(num_classes:int=3,\n","                     seed:int=42):\n","    \"\"\"Creates a ViT-B/16 feature extractor model and transforms.\n","\n","    Args:\n","        num_classes (int, optional): number of target classes. Defaults to 3.\n","        seed (int, optional): random seed value for output layer. Defaults to 42.\n","\n","    Returns:\n","        model (torch.nn.Module): ViT-B/16 feature extractor model.\n","        transforms (torchvision.transforms): ViT-B/16 image transforms.\n","    \"\"\"\n","    # Create ViT_B_16 pretrained weights, transforms and model\n","    weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n","    transforms = weights.transforms()\n","    model = torchvision.models.vit_b_16(weights=weights)\n","\n","    # Freeze all layers in model\n","    for param in model.parameters():\n","        param.requires_grad = False\n","\n","    # Change classifier head to suit our needs (this will be trainable)\n","    torch.manual_seed(seed)\n","    model.heads = nn.Sequential(nn.Linear(in_features=768, # keep this the same as original model\n","                                          out_features=num_classes)) # update to reflect target number of classes\n","\n","    return model, transforms"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j5PDBXTrNMZf","executionInfo":{"status":"ok","timestamp":1721767812219,"user_tz":-60,"elapsed":394,"user":{"displayName":"lucykingee ude","userId":"14499258343167677547"}},"outputId":"758744fc-8248-4b13-ebd6-22026349fc70"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing demos/foodvision_big2/model.py\n"]}]},{"cell_type":"markdown","source":["### Turning our FoodVision Big Gradio app into a Python script (app.py)"],"metadata":{"id":"DneAmQqqNvlI"}},{"cell_type":"code","source":["%%writefile demos/foodvision_big2/app.py\n","### 1. Imports and class names setup ###\n","import gradio as gr\n","import os\n","import torch\n","\n","from model import create_vit_model\n","from timeit import default_timer as timer\n","from typing import Tuple, Dict\n","\n","# Setup class names\n","with open(\"class_names.txt\", \"r\") as f: # reading them in from class_names.txt\n","    class_names = [food_name.strip() for food_name in  f.readlines()]\n","\n","### 2. Model and transforms preparation ###\n","\n","# Create model\n","vit, vit_transforms = create_vit_model(\n","    num_classes=101, # could also use len(class_names)\n",")\n","\n","# Load saved weights\n","vit.load_state_dict(\n","    torch.load(\n","        f=\"pretrained_vit_feature_extractor_food101_20_percent.pth\",\n","        map_location=torch.device(\"cpu\"),  # load to CPU\n","    )\n",")\n","\n","### 3. Predict function ###\n","\n","# Create predict function\n","def predict(img) -> Tuple[Dict, float]:\n","    \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n","    \"\"\"\n","    # Start the timer\n","    start_time = timer()\n","\n","    # Transform the target image and add a batch dimension\n","    img = vit_transforms(img).unsqueeze(0)\n","\n","    # Put model into evaluation mode and turn on inference mode\n","    vit.eval()\n","    with torch.inference_mode():\n","        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n","        pred_probs = torch.softmax(vit(img), dim=1)\n","\n","    # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n","    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n","\n","    # Calculate the prediction time\n","    pred_time = round(timer() - start_time, 5)\n","\n","    # Return the prediction dictionary and prediction time\n","    return pred_labels_and_probs, pred_time\n","\n","### 4. Gradio app ###\n","\n","# Create title, description and article strings\n","title = \"FoodVision Big üçîüëÅ\"\n","description = \"An vit feature extractor computer vision model to classify images of food into 101 different classes\"\n","article = \"Practicing\"\n","\n","# Create examples list from \"examples/\" directory\n","example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n","\n","# Create Gradio interface\n","demo = gr.Interface(\n","    fn=predict,\n","    inputs=gr.Image(type=\"pil\"),\n","    outputs=[\n","        gr.Label(num_top_classes=5, label=\"Predictions\"),\n","        gr.Number(label=\"Prediction time (s)\"),\n","    ],\n","    examples=example_list,\n","    title=title,\n","    description=description,\n","    article=article,\n",")\n","\n","# Launch the app!\n","demo.launch()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fhiq9h6RNrF_","executionInfo":{"status":"ok","timestamp":1721767816613,"user_tz":-60,"elapsed":357,"user":{"displayName":"lucykingee ude","userId":"14499258343167677547"}},"outputId":"8061cde6-0df8-4d2f-fd45-01ea62f9e4ae"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing demos/foodvision_big2/app.py\n"]}]},{"cell_type":"markdown","source":["###  Creating a requirements file for FoodVision Big (requirements.txt)"],"metadata":{"id":"gYZKZuY-PM54"}},{"cell_type":"code","source":["%%writefile demos/foodvision_big2/requirements.txt\n","torch==1.12.0\n","torchvision==0.13.0\n","gradio==3.1.4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P_gh8ji8PJPf","executionInfo":{"status":"ok","timestamp":1721767830909,"user_tz":-60,"elapsed":334,"user":{"displayName":"lucykingee ude","userId":"14499258343167677547"}},"outputId":"0a078ec0-a904-4a3d-ea17-9ad334a6ab9a"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing demos/foodvision_big2/requirements.txt\n"]}]},{"cell_type":"markdown","source":["### Downloading our FoodVision Big app files"],"metadata":{"id":"iRPEIvHEPQ3q"}},{"cell_type":"code","source":["# Zip foodvision_big folder but exclude certain files\n","!cd demos/foodvision_big2 && zip -r ../foodvision_big2.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n","\n","# Download the zipped FoodVision Big app (if running in Google Colab)\n","try:\n","    from google.colab import files\n","    files.download(\"demos/foodvision_big2.zip\")\n","except:\n","    print(\"Not running in Google Colab, can't use google.colab.files.download()\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":138},"id":"Li_UMTeyPWWB","executionInfo":{"status":"ok","timestamp":1721768399442,"user_tz":-60,"elapsed":22279,"user":{"displayName":"lucykingee ude","userId":"14499258343167677547"}},"outputId":"f775cba0-4f10-475f-853e-a14233127adc"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["updating: app.py (deflated 55%)\n","updating: class_names.txt (deflated 48%)\n","updating: examples/ (stored 0%)\n","updating: model.py (deflated 55%)\n","updating: requirements.txt (deflated 4%)\n","  adding: pretrained_vit_feature_extractor_food101_20_percent.pth (deflated 7%)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_58028c89-6550-4a50-9540-ca8b4c2d4658\", \"foodvision_big2.zip\", 319418966)"]},"metadata":{}}]}]}